1. Warmup: Why do we use caches? Please relate this to the von Neumann bottleneck.

Because in von Neumann's model for computer architecture, the processor and memory are connected by a bus which has limited capacity and the speed of processors is significantly faster than that of main memory. The processor would be waiting for a long time for data to be transferred from the main memory to the processor, thus wasting lots of resources. However, after introducing cache as a middle man, the processor can get recently used data way much faster than before, by simply fetching it from the cache without having to send a request back to the main memory and wait. The load of the bus could be decreased as well. Moreover, the cache can fill in the gap of performance difference between main memory and processors, CPU can now ask for data in chunks and take advantage of locality and preload data without having to wait for data delivery every single time.  

2. Apply Amdahl’s law to compute the speedup for the following program if you have (a) 1, (b) 2,(c) 4, (d) 8, (e) 12, (f) 16, and (g) ∞ CPUs. In the following diagram, S portions are sequential and P are parallelizable.

The formula for Amdahl's law is 1/(F + (1 - F) / P)
Every = sign here means "around" truncating except the first digit after the decimal.
a) 1 because it is single core, therefor parallelism doesn't work. Everything is sequential 
b) 1 / (0.33 + (1 - 0.33) / 2) = 1.5  
c) 1 / (0.33 + (1 - 0.33) / 4) = 2.0 
d) 1 / (0.33 + (1 - 0.33) / 8) = 2.4
e) 1 / (0.33 + (1 - 0.33) / 12) = 2.6
f) 1 / (0.33 + (1 - 0.33) / 16) = 2.7
g) 1 / (0.33 + (1 - 0.33) / ∞) = 3.0

3. Explain the difference between concurrency and parallelism with an example: if an operating
system is executing three long-running programs, how would its scheduler execute the programs
concurrently on one core, concurrently on three cores, or in parallel on three cores? Comment on
running the programs in parallel on one core.

The main difference between concurrency and parallelism is that the former concept relates to the ability of running several tasks in a small interval of time on a single core and the later concept refers to the ability of running several tasks simultaneously on more than one core. Assume that there are three long-running programs on a single computer C. If C has one core and concurrently runs these jobs, then one program might run before others or some part of these programs would be scheduled to run first and stop to wait for some other parts of these programs to run and maybe resume later. No two programs are run at the exact same time but they would be finished together in a small interval of time. To the viewer, it might feel like all of these three programs are running together because C actually runs some of one task and some of other tasks and keep executing instructions of these programs alternately to produce the illusion that they are running together. If C has three cores, then it wouldn't make a big difference from having only one core running concurrently. If C has three cores and supports parallelism, then in best cases, all three programs could run literally together, at exactly same time, and instructions by each of these programs could be handled by each core. If C has only one core and supports parallelism, then it is not possible to have the actual simultaneous actions unless other techniques are introduced like multithreading or hyper-threading.  

4. If we can have fast caches on the same die as a CPU core, why bother having main memory at all? Why not just have bigger caches? (There are multiple right answers here; see if you can think of one or more reasons, and explain each one. Guessing one correct answer and several wrong ones is worse than a single correct answer.)

The point here is the tradeoff between speed and space. If a memory storage device is far away from cpu then it would be inevitably slow as electrons take times to run across. And if a device is too big then it is not feasible to be integrated into the same die as the cores. If we have a really big cache, could store as many data as modern RAM, then there would be lots of transistors in a small area (the die), which would produce too much heat to be dissipate. 

5. Relatedly, what’s the point of cache? Does its purpose differ between single- and multi-core
processors (if so, how)? Does its implementation differ between single- and multi-core
processors (if so, how and why)?

The purpose of cache memory is to compensate the speed difference between processors and main memory. With it, processors could fetch recently used data way faster then sending a request to the main memory and next time when a specific data is delivered from the main memory, a chunk of other potentially useful data are also packed up and sent to the cache. This would be a speed-up if processors use them in a short period of time. Cache memory serves as a middle man with a good balance of speed and size in the memory hierarchy. The main purpose of cache in single and multi core machines are the same however in multicore machines, each core would have their own private L1 cache that other cores wouldn't be able to access. This also introduces problems like cache coherence though.

6. You are implementing a server that sends responses to requests, and requires very small
amounts of computation to handle each request. Your application requires many clients and
many servers to work together. Which of the programming models discussed in class are you
likely to use to model this application, and why?

I would choose LogP model because the most important properties I would like to consider for such a system are its latency of communication, the gap between consecutive transmissions. Four most important variables that define a simple LogP model are the most critical points that we concern here. PRAM model is not a good fit because it doesn't predict the performance very accurately but our system is designed to have multiple servers and clients so having a good measurement of performance is vitally important. BSP model is not suitable because most of our requests are really lightweight, having a barrier for each phase would easily be an overhead if designed improperly. Also BSP model is not good for distinguishing between sending 1 message of length m and m messages of length 1, which makes it not a good choice to model our system.
